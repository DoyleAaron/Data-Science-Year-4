{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Change Log\n",
    "\n",
    "#### Dataset & Model Overview\n",
    "\n",
    "- After getting a decent understanding of how a supervised neural network like the one I did in predicting wine quality with a FFNN, I wanted to try working with image recognition and detection.\n",
    "- I am using this dataset (https://www.cs.toronto.edu/~kriz/cifar.html), this dataset is a subset of the 80 million tiny images dataset.\n",
    "- This dataset has 60,000 images with 10 classes and 6000 images per class.\n",
    "- I want this model to be able to correctly classify the class that the image belongs to.\n",
    "- I am planning to use a Convolutional Neural Network (CNN) for this, I chose a CNN because they are very good at pattern recognition and image detection with classification which is exactly what is needed for this task.\n",
    "\n",
    "#### Learnings & Findings\n",
    "\n",
    "- I am extracting the dataset in the way specified here (https://www.cs.toronto.edu/~kriz/cifar.html) alongside extra code to extract and combine them into a singular csv file.\n",
    "- I am going to try the CNN structure similar to what is found here (https://www.analyticsvidhya.com/blog/2020/02/learn-image-classification-cnn-convolutional-neural-networks-3-datasets/), I used Tensorflow for my last model and it worked well so I am going to try and use it again.\n",
    "- I had to learn how the image classification is structured in tensorflow, my article that I referenced above has some information on it but I used some other sites for research such as (https://www.kaggle.com/code/anandhuh/image-classification-using-cnn-for-beginners).\n",
    "- Ran into an issue with how my data was being structured after I changed it to a CSV file. When I was trying to fit the model it was throwing an error because my data was structured incorrectly for the CNN. I was struggling to figure out why but I ended up asking ChatGPT for help here. It told me that the issue is that it was being stored as a 1D array where as the CNN model requires it to be a 3D image. So it converted it to a numpy arrray and then reshaped the data so that it would fit the model better.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I am just importing the necessary libraries and technologies that I may need to build this model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file saved successfully!\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "\n",
    "def unpickle(file):\n",
    "    with open(file, \"rb\") as fo:\n",
    "        batch = pickle.load(fo, encoding=\"bytes\")\n",
    "    return batch\n",
    "\n",
    "\n",
    "data_list = []\n",
    "labels_list = []\n",
    "\n",
    "# Loop through all batch files\n",
    "for i in range(1, 6):\n",
    "    file = f\"data_batch_{i}\"  # Modify if needed\n",
    "    batch = unpickle(file)\n",
    "\n",
    "    data_list.append(batch[b\"data\"])  # Image data\n",
    "    labels_list.append(batch[b\"labels\"])  # Labels\n",
    "\n",
    "# Convert lists to NumPy arrays\n",
    "data = np.vstack(data_list)\n",
    "labels = np.hstack(labels_list)\n",
    "\n",
    "# Convert to DataFrame\n",
    "columns = [f\"pixel_{i}\" for i in range(data.shape[1])]  # Column names for pixels\n",
    "df = pd.DataFrame(data, columns=columns)\n",
    "df.insert(0, \"label\", labels)  # Insert labels as the first column\n",
    "\n",
    "# Save to CSV\n",
    "df.to_csv(\"cifar10_data.csv\", index=False)\n",
    "print(\"CSV file saved successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ChatGPT generated this for me as I wanted to store this as a CSV file rather than the way they were initially being handled.\n",
    "\n",
    "What it is doing is, looping through each of the files which has 10,000 images in it and labels of what each image is of so the model can begin to build up an idea of what should be in each class, it then extracts the data and stores it in a pandas dataframe which can then be converted to a singular csv file.\n",
    "\n",
    "I am now going to turn the test batch into a csv file as well.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file saved successfully!\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "\n",
    "def unpickle(file):\n",
    "    with open(file, \"rb\") as fo:\n",
    "        batch = pickle.load(fo, encoding=\"bytes\")\n",
    "    return batch\n",
    "\n",
    "\n",
    "data_list = []\n",
    "labels_list = []\n",
    "\n",
    "file = \"test_batch\"  # Modify if needed\n",
    "batch = unpickle(file)\n",
    "\n",
    "data_list.append(batch[b\"data\"])  # Image data\n",
    "labels_list.append(batch[b\"labels\"])  # Labels\n",
    "\n",
    "# Convert lists to NumPy arrays\n",
    "data = np.vstack(data_list)\n",
    "labels = np.hstack(labels_list)\n",
    "\n",
    "# Convert to DataFrame\n",
    "columns = [f\"pixel_{i}\" for i in range(data.shape[1])]  # Column names for pixels\n",
    "df = pd.DataFrame(data, columns=columns)\n",
    "df.insert(0, \"label\", labels)  # Insert labels as the first column\n",
    "\n",
    "# Save to CSV\n",
    "df.to_csv(\"cifar10_test_data.csv\", index=False)\n",
    "print(\"CSV file saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (50000, 32, 32, 3)\n",
      "y shape: (50000,)\n",
      "X_test shape: (10000, 32, 32, 3)\n",
      "y_test shape: (10000,)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"cifar10_data.csv\")\n",
    "test_df = pd.read_csv(\"cifar10_test_data.csv\")\n",
    "\n",
    "# Separate features and labels\n",
    "X = df.drop(\"label\", axis=1).values  # Convert to NumPy array\n",
    "y = df[\"label\"].values  # Convert labels to NumPy array\n",
    "\n",
    "# Reshape X to match CNN input (num_samples, 32, 32, 3)\n",
    "X = X.reshape(-1, 32, 32, 3)\n",
    "\n",
    "# Do the same for test data\n",
    "X_test = test_df.drop(\"label\", axis=1).values  # Convert to NumPy array\n",
    "y_test = test_df[\"label\"].values  # Convert labels to NumPy array\n",
    "\n",
    "# Reshape X_test to match CNN input\n",
    "X_test = X_test.reshape(-1, 32, 32, 3)\n",
    "\n",
    "# Normalize pixel values (important for CNNs)\n",
    "X = X / 255.0\n",
    "X_test = X_test / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I'm just assigning both datasets and the target variables for both the training and test dataset.\n",
    "\n",
    "My next step is now to add the layers to the model that I will need for the classification.\n",
    "\n",
    "##### Update\n",
    "\n",
    "I had to get ChatGPT to help me retructure my dataframe. Full explanation in the change log.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(32, (3, 3), activation=\"relu\", input_shape=(32, 32, 3)))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation=\"relu\"))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation=\"relu\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the most popular structure that I have seen online. After doing some more research about what the functions that are being called are doing it is as follows:\n",
    "\n",
    "A Sequential model is being used as each layer is using the last layers output as its input, this is important for something like image clasification because we arre filtering and condensing the image so it needs to be executed in sequential order.\n",
    "\n",
    "The Conv2D function is used to add a convolutional layer, what this does is it sets an amount of features which in this case is 32, and these will scan over the image in a 3x3 grid looking for key edges or features that are standing out. Then I have the ReLU activation function which just turns negative values into 0. Finally the input shape is just telling the model what the shape of my image is so you have to specify the height, width and colourscale so 3 is RGB for example.\n",
    "\n",
    "The MaxPooling2d function is used to downsize the image that we have. So how this works is it will move across in a 2x2 grid and look for the square with the most important features in it and it removes the rest, this is so it is focusing on the key information and not background and unimportant parts of the image.\n",
    "\n",
    "This process is then repeated a couple more times so that the model can continue to try and correctly identify the correct image.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(64, activation=\"relu\"))\n",
    "model.add(layers.Dense(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=[\"accuracy\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 10ms/step - accuracy: 0.3326 - loss: 1.8263 - val_accuracy: 0.4961 - val_loss: 1.3855\n",
      "Epoch 2/10\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 11ms/step - accuracy: 0.5050 - loss: 1.3986 - val_accuracy: 0.5323 - val_loss: 1.3028\n",
      "Epoch 3/10\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 10ms/step - accuracy: 0.5592 - loss: 1.2424 - val_accuracy: 0.5666 - val_loss: 1.2101\n",
      "Epoch 4/10\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 10ms/step - accuracy: 0.5951 - loss: 1.1482 - val_accuracy: 0.5682 - val_loss: 1.2232\n",
      "Epoch 5/10\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 10ms/step - accuracy: 0.6191 - loss: 1.0799 - val_accuracy: 0.6031 - val_loss: 1.1188\n",
      "Epoch 6/10\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 10ms/step - accuracy: 0.6381 - loss: 1.0239 - val_accuracy: 0.5668 - val_loss: 1.2472\n",
      "Epoch 7/10\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 10ms/step - accuracy: 0.6524 - loss: 0.9778 - val_accuracy: 0.6054 - val_loss: 1.1299\n",
      "Epoch 8/10\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 10ms/step - accuracy: 0.6686 - loss: 0.9385 - val_accuracy: 0.6181 - val_loss: 1.1028\n",
      "Epoch 9/10\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 11ms/step - accuracy: 0.6828 - loss: 0.8998 - val_accuracy: 0.6059 - val_loss: 1.1525\n",
      "Epoch 10/10\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 11ms/step - accuracy: 0.6870 - loss: 0.8755 - val_accuracy: 0.6105 - val_loss: 1.1519\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x3641ca8d0>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "model.fit(X, y, epochs=10, validation_data=(X_test, y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
