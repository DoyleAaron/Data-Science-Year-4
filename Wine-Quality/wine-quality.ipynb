{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Change Log\n",
    "\n",
    "\n",
    "#### Overview\n",
    "- This model aims to be able to predict the quality of wine based off of different factors about the wine.\n",
    "- I obtained my dataset from the University of California Irvine's website (https://archive.ics.uci.edu/dataset/186/wine+quality)\n",
    "- There are 11 columns about the wine and it's makeup and then a quality rating at the end for my target value.\n",
    "- The red wine dataset has 1600 entries and the white wine dataset has 4900 entries.\n",
    "- This is a relatively small dataset but I wanted to use a simple enough dataset and prediction whilst I get used to using Neural Networks and figuring out how they work.\n",
    "- I am planning to use a Feedforward Neural Network for this as it is a simple regression task and it should be a good fit for the data that I have.\n",
    "\n",
    "\n",
    "#### Learnings & Findings\n",
    "- I was reading the tensorflow docs here (https://www.tensorflow.org/tutorials/quickstart/beginner) and a GeeksForGeeks artice here (https://www.geeksforgeeks.org/feedforward-neural-network/) to get an idea of how to structure the code for this model.\n",
    "- I have always used Sci-Kit learn for all of my models previously, however I was doing some research and found that Tensorflow seems to be a better fit for Neural Networks as it gives you a lot more control over the model in comparison to Sci-Kit Learns version.\n",
    "- I would like to see if there is a difference in what makes a white wine good quality in comparison to red wine, this is something I'd like to investigate further on in the notebook, I want to run this model on Red, White and a combination of the two to see the results.\n",
    "- Learned about adam optimiser from here (https://www.geeksforgeeks.org/adam-optimizer-in-tensorflow/)\n",
    "- I have a MAE rating of 0.57 now so I want to see if I can improve this by using Standard Scaling\n",
    "- I then moved on to replicating the same process with the red wine dataset, with even better success at a 0.51 MAE score.\n",
    "- I then wanted to see what the score would be like for the combination of both and this was also a success.\n",
    "\n",
    "\n",
    "#### Results\n",
    "- I now have a good and much clearer understanding of how even a simple enough Neural Network works and how it is structured.\n",
    "- These datasets worked well in making predictions in the end with them all only being about .5 off in mst scenarios on the quality score.\n",
    "- I can now take these learnings and take on a more complex model with more obscure data to challenge myself in using Neural Networks.\n",
    "- Red wine and Combined ended up being a little bit better on the predictions but not by a massive amount.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
      "0            7.0              0.27         0.36            20.7      0.045   \n",
      "1            6.3              0.30         0.34             1.6      0.049   \n",
      "2            8.1              0.28         0.40             6.9      0.050   \n",
      "3            7.2              0.23         0.32             8.5      0.058   \n",
      "4            7.2              0.23         0.32             8.5      0.058   \n",
      "\n",
      "   free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
      "0                 45.0                 170.0   1.0010  3.00       0.45   \n",
      "1                 14.0                 132.0   0.9940  3.30       0.49   \n",
      "2                 30.0                  97.0   0.9951  3.26       0.44   \n",
      "3                 47.0                 186.0   0.9956  3.19       0.40   \n",
      "4                 47.0                 186.0   0.9956  3.19       0.40   \n",
      "\n",
      "   alcohol  quality  \n",
      "0      8.8        6  \n",
      "1      9.5        6  \n",
      "2     10.1        6  \n",
      "3      9.9        6  \n",
      "4      9.9        6  \n",
      "   fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
      "0            7.4              0.70         0.00             1.9      0.076   \n",
      "1            7.8              0.88         0.00             2.6      0.098   \n",
      "2            7.8              0.76         0.04             2.3      0.092   \n",
      "3           11.2              0.28         0.56             1.9      0.075   \n",
      "4            7.4              0.70         0.00             1.9      0.076   \n",
      "\n",
      "   free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
      "0                 11.0                  34.0   0.9978  3.51       0.56   \n",
      "1                 25.0                  67.0   0.9968  3.20       0.68   \n",
      "2                 15.0                  54.0   0.9970  3.26       0.65   \n",
      "3                 17.0                  60.0   0.9980  3.16       0.58   \n",
      "4                 11.0                  34.0   0.9978  3.51       0.56   \n",
      "\n",
      "   alcohol  quality  \n",
      "0      9.4        5  \n",
      "1      9.8        5  \n",
      "2      9.8        5  \n",
      "3      9.8        6  \n",
      "4      9.4        5  \n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "white_df = pd.read_csv('winequality-white.csv', sep=';')\n",
    "red_df = pd.read_csv('winequality-red.csv', sep=';')\n",
    "\n",
    "print(white_df.head())\n",
    "print(red_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am just importing my two datasets here using pandas and importing the necessary libraries for later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = white_df.drop('quality', axis=1)\n",
    "y = white_df['quality']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm just starting off with white at random to see how it goes, I will eventually be doing this model on all of the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential([\n",
    "    keras.layers.Input(shape=(X_train.shape[1],)),\n",
    "    keras.layers.Dense(64, activation='relu'),\n",
    "    keras.layers.Dense(32, activation='relu'),\n",
    "    keras.layers.Dense(1, activation='linear')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I am defining the model using Keras and Tensorflow.\n",
    "\n",
    "Using Sequential here tells it that we are using a Feed Forward Neural Network. Firstly I added my input layer which just contains my training X data. Then I am creating 2 hidden layers on top of Tensorflows already built model with 64 neurons and then the next one is the same but with 32 neurons. The relu activation function keeps the positive values but turns the negative ones into 0. The final layer I added then is the output layer which just has one neuron and I used linear becasue this is a regression model rather than classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='mse', metrics=['mae'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this code block I am compiling the model. An optimiser controls how the model updates the weights for the Neural Network. The adam optimiser is usually used becuase it adjusts the learning rates automatically.\n",
    "\n",
    "I am then using Mean Square Error and Mean Absolute error as these are very good for regression as they track how much error is occuring in the predictions that the model is making."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 951us/step - loss: 18.0223 - mae: 2.8530 - val_loss: 1.0642 - val_mae: 0.8004\n",
      "Epoch 2/100\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 671us/step - loss: 0.7668 - mae: 0.6834 - val_loss: 0.6459 - val_mae: 0.6253\n",
      "Epoch 3/100\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 665us/step - loss: 0.7435 - mae: 0.6764 - val_loss: 0.6571 - val_mae: 0.6319\n",
      "Epoch 4/100\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 665us/step - loss: 0.6736 - mae: 0.6427 - val_loss: 0.6431 - val_mae: 0.6277\n",
      "Epoch 5/100\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 667us/step - loss: 0.6657 - mae: 0.6253 - val_loss: 0.6236 - val_mae: 0.6170\n",
      "Epoch 6/100\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 670us/step - loss: 0.7041 - mae: 0.6524 - val_loss: 0.6539 - val_mae: 0.6369\n",
      "Epoch 7/100\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 668us/step - loss: 0.6596 - mae: 0.6354 - val_loss: 0.8880 - val_mae: 0.7370\n",
      "Epoch 8/100\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 671us/step - loss: 0.7306 - mae: 0.6730 - val_loss: 0.7512 - val_mae: 0.6706\n",
      "Epoch 9/100\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 675us/step - loss: 0.8951 - mae: 0.7438 - val_loss: 0.8290 - val_mae: 0.7329\n",
      "Epoch 10/100\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 662us/step - loss: 0.7282 - mae: 0.6680 - val_loss: 0.8485 - val_mae: 0.7414\n",
      "Epoch 11/100\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 652us/step - loss: 0.8454 - mae: 0.7257 - val_loss: 0.6002 - val_mae: 0.6045\n",
      "Epoch 12/100\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 672us/step - loss: 0.6253 - mae: 0.6161 - val_loss: 1.1739 - val_mae: 0.8697\n",
      "Epoch 13/100\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 678us/step - loss: 0.7739 - mae: 0.6910 - val_loss: 0.6033 - val_mae: 0.6055\n",
      "Epoch 14/100\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 665us/step - loss: 0.6306 - mae: 0.6242 - val_loss: 0.5875 - val_mae: 0.5991\n",
      "Epoch 15/100\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 665us/step - loss: 0.6528 - mae: 0.6401 - val_loss: 1.0507 - val_mae: 0.8101\n",
      "Epoch 16/100\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 687us/step - loss: 0.8030 - mae: 0.7125 - val_loss: 0.6653 - val_mae: 0.6288\n",
      "Epoch 17/100\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 675us/step - loss: 0.7328 - mae: 0.6614 - val_loss: 1.0034 - val_mae: 0.8203\n",
      "Epoch 18/100\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 649us/step - loss: 0.7551 - mae: 0.6815 - val_loss: 0.5862 - val_mae: 0.6046\n",
      "Epoch 19/100\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 670us/step - loss: 0.6716 - mae: 0.6457 - val_loss: 0.6041 - val_mae: 0.6121\n",
      "Epoch 20/100\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 667us/step - loss: 0.6108 - mae: 0.6149 - val_loss: 0.6138 - val_mae: 0.6125\n",
      "Epoch 21/100\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 665us/step - loss: 0.6788 - mae: 0.6458 - val_loss: 0.8042 - val_mae: 0.6926\n",
      "Epoch 22/100\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 660us/step - loss: 0.6961 - mae: 0.6573 - val_loss: 0.7104 - val_mae: 0.6691\n",
      "Epoch 23/100\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 662us/step - loss: 0.6294 - mae: 0.6218 - val_loss: 0.6867 - val_mae: 0.6579\n",
      "Epoch 24/100\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 665us/step - loss: 0.6541 - mae: 0.6289 - val_loss: 0.8090 - val_mae: 0.6959\n",
      "Epoch 25/100\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 670us/step - loss: 0.6615 - mae: 0.6309 - val_loss: 0.5892 - val_mae: 0.5958\n",
      "Epoch 26/100\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 666us/step - loss: 0.6461 - mae: 0.6353 - val_loss: 0.7365 - val_mae: 0.6627\n",
      "Epoch 27/100\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 650us/step - loss: 0.7638 - mae: 0.6865 - val_loss: 0.6348 - val_mae: 0.6117\n",
      "Epoch 28/100\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 665us/step - loss: 0.6033 - mae: 0.6071 - val_loss: 0.6371 - val_mae: 0.6169\n",
      "Epoch 29/100\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 663us/step - loss: 0.5712 - mae: 0.5904 - val_loss: 0.5971 - val_mae: 0.5989\n",
      "Epoch 30/100\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 649us/step - loss: 0.5954 - mae: 0.6052 - val_loss: 0.7430 - val_mae: 0.6630\n",
      "Epoch 31/100\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 665us/step - loss: 0.6271 - mae: 0.6157 - val_loss: 0.9087 - val_mae: 0.7482\n",
      "Epoch 32/100\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 655us/step - loss: 0.5961 - mae: 0.5978 - val_loss: 0.5844 - val_mae: 0.5948\n",
      "Epoch 33/100\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 669us/step - loss: 0.6520 - mae: 0.6419 - val_loss: 0.5891 - val_mae: 0.6010\n",
      "Epoch 34/100\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 706us/step - loss: 0.6027 - mae: 0.6088 - val_loss: 0.6887 - val_mae: 0.6375\n",
      "Epoch 35/100\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 645us/step - loss: 0.6369 - mae: 0.6268 - val_loss: 0.7299 - val_mae: 0.6623\n",
      "Epoch 36/100\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 634us/step - loss: 0.6361 - mae: 0.6236 - val_loss: 0.7137 - val_mae: 0.6476\n",
      "Epoch 37/100\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 643us/step - loss: 0.6561 - mae: 0.6301 - val_loss: 0.6781 - val_mae: 0.6354\n",
      "Epoch 38/100\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 677us/step - loss: 0.5829 - mae: 0.5903 - val_loss: 0.6223 - val_mae: 0.6168\n",
      "Epoch 39/100\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 637us/step - loss: 0.6167 - mae: 0.6077 - val_loss: 0.7516 - val_mae: 0.6870\n",
      "Epoch 40/100\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 895us/step - loss: 0.5916 - mae: 0.6002 - val_loss: 0.6376 - val_mae: 0.6164\n",
      "Epoch 41/100\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 668us/step - loss: 0.6046 - mae: 0.6089 - val_loss: 0.6201 - val_mae: 0.6145\n",
      "Epoch 42/100\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 662us/step - loss: 0.6359 - mae: 0.6207 - val_loss: 0.5978 - val_mae: 0.6032\n",
      "Epoch 43/100\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 679us/step - loss: 0.5849 - mae: 0.6003 - val_loss: 0.5875 - val_mae: 0.5940\n",
      "Epoch 44/100\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 663us/step - loss: 0.5799 - mae: 0.5984 - val_loss: 0.6294 - val_mae: 0.6113\n",
      "Epoch 45/100\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 668us/step - loss: 0.5856 - mae: 0.5960 - val_loss: 0.6630 - val_mae: 0.6229\n",
      "Epoch 46/100\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 662us/step - loss: 0.6639 - mae: 0.6383 - val_loss: 0.5816 - val_mae: 0.5912\n",
      "Epoch 47/100\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 668us/step - loss: 0.5771 - mae: 0.5916 - val_loss: 0.6882 - val_mae: 0.6321\n",
      "Epoch 48/100\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 643us/step - loss: 0.6298 - mae: 0.6201 - val_loss: 0.5840 - val_mae: 0.5935\n",
      "Epoch 49/100\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 662us/step - loss: 0.6003 - mae: 0.6070 - val_loss: 0.6098 - val_mae: 0.6040\n",
      "Epoch 50/100\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 667us/step - loss: 0.5900 - mae: 0.5922 - val_loss: 0.5984 - val_mae: 0.5930\n",
      "Epoch 51/100\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 672us/step - loss: 0.6271 - mae: 0.6235 - val_loss: 0.5870 - val_mae: 0.5934\n",
      "Epoch 52/100\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 691us/step - loss: 0.5669 - mae: 0.5910 - val_loss: 0.7179 - val_mae: 0.6525\n",
      "Epoch 53/100\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 665us/step - loss: 0.6035 - mae: 0.6099 - val_loss: 0.7189 - val_mae: 0.6561\n",
      "Epoch 54/100\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 669us/step - loss: 0.6292 - mae: 0.6223 - val_loss: 0.6151 - val_mae: 0.6127\n",
      "Epoch 55/100\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 671us/step - loss: 0.5426 - mae: 0.5797 - val_loss: 0.5993 - val_mae: 0.5976\n",
      "Epoch 56/100\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 662us/step - loss: 0.5953 - mae: 0.5956 - val_loss: 0.5869 - val_mae: 0.5959\n",
      "Epoch 57/100\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 651us/step - loss: 0.5927 - mae: 0.6039 - val_loss: 0.6083 - val_mae: 0.6086\n",
      "Epoch 58/100\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 674us/step - loss: 0.6074 - mae: 0.6108 - val_loss: 0.5972 - val_mae: 0.5995\n",
      "Epoch 59/100\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 652us/step - loss: 0.6185 - mae: 0.6087 - val_loss: 0.5945 - val_mae: 0.5934\n",
      "Epoch 60/100\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 668us/step - loss: 0.5833 - mae: 0.5953 - val_loss: 0.5757 - val_mae: 0.5881\n",
      "Epoch 61/100\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 671us/step - loss: 0.5785 - mae: 0.5933 - val_loss: 0.9049 - val_mae: 0.7395\n",
      "Epoch 62/100\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 665us/step - loss: 0.5796 - mae: 0.5967 - val_loss: 0.6315 - val_mae: 0.6090\n",
      "Epoch 63/100\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 664us/step - loss: 0.5593 - mae: 0.5868 - val_loss: 0.5977 - val_mae: 0.5965\n",
      "Epoch 64/100\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 670us/step - loss: 0.5358 - mae: 0.5749 - val_loss: 0.6351 - val_mae: 0.6235\n",
      "Epoch 65/100\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 660us/step - loss: 0.5897 - mae: 0.5936 - val_loss: 0.5864 - val_mae: 0.5918\n",
      "Epoch 66/100\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 667us/step - loss: 0.5636 - mae: 0.5899 - val_loss: 0.6123 - val_mae: 0.6106\n",
      "Epoch 67/100\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 669us/step - loss: 0.5885 - mae: 0.6012 - val_loss: 0.5818 - val_mae: 0.5899\n",
      "Epoch 68/100\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 666us/step - loss: 0.6060 - mae: 0.6054 - val_loss: 0.5815 - val_mae: 0.5908\n",
      "Epoch 69/100\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 668us/step - loss: 0.5880 - mae: 0.6033 - val_loss: 0.5862 - val_mae: 0.5938\n",
      "Epoch 70/100\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 679us/step - loss: 0.6027 - mae: 0.6055 - val_loss: 0.5908 - val_mae: 0.5981\n",
      "Epoch 71/100\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 663us/step - loss: 0.5504 - mae: 0.5765 - val_loss: 0.6048 - val_mae: 0.6012\n",
      "Epoch 72/100\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 832us/step - loss: 0.5286 - mae: 0.5732 - val_loss: 0.7828 - val_mae: 0.7077\n",
      "Epoch 73/100\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 673us/step - loss: 0.5633 - mae: 0.5917 - val_loss: 0.6731 - val_mae: 0.6319\n",
      "Epoch 74/100\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 671us/step - loss: 0.5439 - mae: 0.5773 - val_loss: 0.5843 - val_mae: 0.5912\n",
      "Epoch 75/100\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 642us/step - loss: 0.5659 - mae: 0.5875 - val_loss: 0.6061 - val_mae: 0.6009\n",
      "Epoch 76/100\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 653us/step - loss: 0.6054 - mae: 0.6104 - val_loss: 0.6117 - val_mae: 0.6029\n",
      "Epoch 77/100\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 664us/step - loss: 0.5693 - mae: 0.5867 - val_loss: 0.5856 - val_mae: 0.5967\n",
      "Epoch 78/100\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 662us/step - loss: 0.5594 - mae: 0.5917 - val_loss: 0.5865 - val_mae: 0.5914\n",
      "Epoch 79/100\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 700us/step - loss: 0.5524 - mae: 0.5787 - val_loss: 0.5905 - val_mae: 0.5986\n",
      "Epoch 80/100\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 668us/step - loss: 0.5419 - mae: 0.5821 - val_loss: 0.7119 - val_mae: 0.6654\n",
      "Epoch 81/100\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 888us/step - loss: 0.5820 - mae: 0.5967 - val_loss: 0.5837 - val_mae: 0.5951\n",
      "Epoch 82/100\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 665us/step - loss: 0.5530 - mae: 0.5826 - val_loss: 0.6018 - val_mae: 0.5985\n",
      "Epoch 83/100\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 674us/step - loss: 0.6151 - mae: 0.6158 - val_loss: 0.5956 - val_mae: 0.5960\n",
      "Epoch 84/100\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 662us/step - loss: 0.5662 - mae: 0.5928 - val_loss: 0.5814 - val_mae: 0.5911\n",
      "Epoch 85/100\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 670us/step - loss: 0.5676 - mae: 0.5880 - val_loss: 0.6348 - val_mae: 0.6209\n",
      "Epoch 86/100\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 667us/step - loss: 0.5683 - mae: 0.5878 - val_loss: 0.6955 - val_mae: 0.6577\n",
      "Epoch 87/100\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 716us/step - loss: 0.6165 - mae: 0.6040 - val_loss: 0.5708 - val_mae: 0.5850\n",
      "Epoch 88/100\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 690us/step - loss: 0.5594 - mae: 0.5853 - val_loss: 0.5786 - val_mae: 0.5869\n",
      "Epoch 89/100\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 646us/step - loss: 0.5384 - mae: 0.5751 - val_loss: 0.6137 - val_mae: 0.6087\n",
      "Epoch 90/100\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 690us/step - loss: 0.5697 - mae: 0.5893 - val_loss: 0.5829 - val_mae: 0.5922\n",
      "Epoch 91/100\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 683us/step - loss: 0.5412 - mae: 0.5788 - val_loss: 0.5707 - val_mae: 0.5865\n",
      "Epoch 92/100\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 674us/step - loss: 0.5400 - mae: 0.5806 - val_loss: 0.5989 - val_mae: 0.5963\n",
      "Epoch 93/100\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 737us/step - loss: 0.5370 - mae: 0.5780 - val_loss: 0.6160 - val_mae: 0.6126\n",
      "Epoch 94/100\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 735us/step - loss: 0.5619 - mae: 0.5846 - val_loss: 0.7307 - val_mae: 0.6813\n",
      "Epoch 95/100\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 638us/step - loss: 0.5521 - mae: 0.5831 - val_loss: 0.5754 - val_mae: 0.5878\n",
      "Epoch 96/100\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 706us/step - loss: 0.5304 - mae: 0.5721 - val_loss: 0.5689 - val_mae: 0.5833\n",
      "Epoch 97/100\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 744us/step - loss: 0.5194 - mae: 0.5633 - val_loss: 0.6094 - val_mae: 0.6085\n",
      "Epoch 98/100\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 882us/step - loss: 0.5718 - mae: 0.5905 - val_loss: 0.5821 - val_mae: 0.5892\n",
      "Epoch 99/100\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 701us/step - loss: 0.5580 - mae: 0.5868 - val_loss: 0.5836 - val_mae: 0.5902\n",
      "Epoch 100/100\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 776us/step - loss: 0.5423 - mae: 0.5796 - val_loss: 0.5721 - val_mae: 0.5859\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x178deff90>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=100, batch_size=16, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this code block I am now fitting my model to the data and it gives me back the results for each of the specified metrics that I gave it beforehand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 748us/step - loss: 0.5459 - mae: 0.5856\n",
      "Model MAE: 0.5759\n"
     ]
    }
   ],
   "source": [
    "loss, mae = model.evaluate(X_test, y_test)\n",
    "print(f\"Model MAE: {mae:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ChatGPT gave me this block just to check the overall MAE for the model which is at 0.57, this means that on average it is 0.57 away from the actual rating which on a scale of 1 to 10 is quite a good rating but I'd like to see if I can improve it.\n",
    "\n",
    "My first step that I'm going to take to see if it improves it is using Standard Scaling, this is because features in this dataset are scaled differently and the mdel might be able to predict better if they are all scaled similarly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X = white_df.drop('quality', axis=1)\n",
    "y = white_df['quality']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I am scaling the x training and test values to see if this will help with the accuracy rating of my model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential([\n",
    "    keras.layers.Input(shape=(X_train.shape[1],)),\n",
    "    keras.layers.Dense(64, activation='relu'),\n",
    "    keras.layers.Dense(32, activation='relu'),\n",
    "    keras.layers.Dense(1, activation='linear')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='mse', metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 949us/step - loss: 15.4253 - mae: 3.3969 - val_loss: 2.2578 - val_mae: 1.1803\n",
      "Epoch 2/100\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 675us/step - loss: 2.1597 - mae: 1.1228 - val_loss: 1.4029 - val_mae: 0.9099\n",
      "Epoch 3/100\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 706us/step - loss: 1.2295 - mae: 0.8644 - val_loss: 0.9535 - val_mae: 0.7487\n",
      "Epoch 4/100\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 643us/step - loss: 0.8830 - mae: 0.7260 - val_loss: 0.7221 - val_mae: 0.6483\n",
      "Epoch 5/100\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 653us/step - loss: 0.6984 - mae: 0.6483 - val_loss: 0.6225 - val_mae: 0.6028\n",
      "Epoch 6/100\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 634us/step - loss: 0.5895 - mae: 0.5950 - val_loss: 0.5848 - val_mae: 0.5956\n",
      "Epoch 7/100\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 638us/step - loss: 0.5323 - mae: 0.5649 - val_loss: 0.5554 - val_mae: 0.5763\n",
      "Epoch 8/100\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 631us/step - loss: 0.5006 - mae: 0.5538 - val_loss: 0.5651 - val_mae: 0.5759\n",
      "Epoch 9/100\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 627us/step - loss: 0.4876 - mae: 0.5511 - val_loss: 0.5439 - val_mae: 0.5647\n",
      "Epoch 10/100\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 631us/step - loss: 0.5072 - mae: 0.5593 - val_loss: 0.5762 - val_mae: 0.5821\n",
      "Epoch 11/100\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 634us/step - loss: 0.4770 - mae: 0.5397 - val_loss: 0.5264 - val_mae: 0.5555\n",
      "Epoch 12/100\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 627us/step - loss: 0.4430 - mae: 0.5260 - val_loss: 0.5328 - val_mae: 0.5600\n",
      "Epoch 13/100\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 633us/step - loss: 0.4723 - mae: 0.5413 - val_loss: 0.5711 - val_mae: 0.5826\n",
      "Epoch 14/100\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 638us/step - loss: 0.4593 - mae: 0.5375 - val_loss: 0.5640 - val_mae: 0.5782\n",
      "Epoch 15/100\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 632us/step - loss: 0.4902 - mae: 0.5480 - val_loss: 0.5203 - val_mae: 0.5482\n",
      "Epoch 16/100\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 656us/step - loss: 0.4455 - mae: 0.5237 - val_loss: 0.5052 - val_mae: 0.5413\n",
      "Epoch 17/100\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 690us/step - loss: 0.4701 - mae: 0.5283 - val_loss: 0.5172 - val_mae: 0.5495\n",
      "Epoch 18/100\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 700us/step - loss: 0.4741 - mae: 0.5426 - val_loss: 0.5259 - val_mae: 0.5554\n",
      "Epoch 19/100\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 635us/step - loss: 0.4350 - mae: 0.5151 - val_loss: 0.5242 - val_mae: 0.5547\n",
      "Epoch 20/100\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 645us/step - loss: 0.4494 - mae: 0.5274 - val_loss: 0.5155 - val_mae: 0.5490\n",
      "Epoch 21/100\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 656us/step - loss: 0.4658 - mae: 0.5281 - val_loss: 0.5110 - val_mae: 0.5439\n",
      "Epoch 22/100\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 645us/step - loss: 0.4141 - mae: 0.5037 - val_loss: 0.5384 - val_mae: 0.5626\n",
      "Epoch 23/100\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 676us/step - loss: 0.4257 - mae: 0.5068 - val_loss: 0.5176 - val_mae: 0.5458\n",
      "Epoch 24/100\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 686us/step - loss: 0.4353 - mae: 0.5165 - val_loss: 0.5113 - val_mae: 0.5455\n",
      "Epoch 25/100\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 683us/step - loss: 0.4385 - mae: 0.5054 - val_loss: 0.5148 - val_mae: 0.5460\n",
      "Epoch 26/100\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 650us/step - loss: 0.4045 - mae: 0.4945 - val_loss: 0.5218 - val_mae: 0.5575\n",
      "Epoch 27/100\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 635us/step - loss: 0.4109 - mae: 0.5014 - val_loss: 0.5377 - val_mae: 0.5637\n",
      "Epoch 28/100\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 695us/step - loss: 0.4391 - mae: 0.5237 - val_loss: 0.5364 - val_mae: 0.5598\n",
      "Epoch 29/100\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 707us/step - loss: 0.4077 - mae: 0.5009 - val_loss: 0.5200 - val_mae: 0.5492\n",
      "Epoch 30/100\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 675us/step - loss: 0.4062 - mae: 0.5031 - val_loss: 0.5272 - val_mae: 0.5526\n",
      "Epoch 31/100\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 697us/step - loss: 0.3993 - mae: 0.4962 - val_loss: 0.5191 - val_mae: 0.5505\n",
      "Epoch 32/100\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 977us/step - loss: 0.4147 - mae: 0.5020 - val_loss: 0.5086 - val_mae: 0.5482\n",
      "Epoch 33/100\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 660us/step - loss: 0.4094 - mae: 0.4985 - val_loss: 0.5523 - val_mae: 0.5745\n",
      "Epoch 34/100\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 643us/step - loss: 0.4301 - mae: 0.5093 - val_loss: 0.5192 - val_mae: 0.5523\n",
      "Epoch 35/100\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 647us/step - loss: 0.3858 - mae: 0.4850 - val_loss: 0.5295 - val_mae: 0.5577\n",
      "Epoch 36/100\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 666us/step - loss: 0.4195 - mae: 0.5085 - val_loss: 0.5099 - val_mae: 0.5415\n",
      "Epoch 37/100\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 674us/step - loss: 0.4127 - mae: 0.4962 - val_loss: 0.5366 - val_mae: 0.5618\n",
      "Epoch 38/100\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 652us/step - loss: 0.3817 - mae: 0.4829 - val_loss: 0.5338 - val_mae: 0.5634\n",
      "Epoch 39/100\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 673us/step - loss: 0.4083 - mae: 0.4968 - val_loss: 0.5207 - val_mae: 0.5527\n",
      "Epoch 40/100\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 669us/step - loss: 0.3867 - mae: 0.4861 - val_loss: 0.5305 - val_mae: 0.5510\n",
      "Epoch 41/100\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 648us/step - loss: 0.3704 - mae: 0.4727 - val_loss: 0.5259 - val_mae: 0.5574\n",
      "Epoch 42/100\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 640us/step - loss: 0.3756 - mae: 0.4839 - val_loss: 0.5421 - val_mae: 0.5661\n",
      "Epoch 43/100\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 631us/step - loss: 0.3805 - mae: 0.4810 - val_loss: 0.5173 - val_mae: 0.5459\n",
      "Epoch 44/100\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 632us/step - loss: 0.3858 - mae: 0.4864 - val_loss: 0.5325 - val_mae: 0.5591\n",
      "Epoch 45/100\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 626us/step - loss: 0.3700 - mae: 0.4716 - val_loss: 0.5397 - val_mae: 0.5644\n",
      "Epoch 46/100\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 632us/step - loss: 0.3547 - mae: 0.4715 - val_loss: 0.5236 - val_mae: 0.5567\n",
      "Epoch 47/100\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 628us/step - loss: 0.3856 - mae: 0.4886 - val_loss: 0.5135 - val_mae: 0.5472\n",
      "Epoch 48/100\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 707us/step - loss: 0.3551 - mae: 0.4678 - val_loss: 0.5162 - val_mae: 0.5507\n",
      "Epoch 49/100\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 639us/step - loss: 0.3572 - mae: 0.4684 - val_loss: 0.5663 - val_mae: 0.5756\n",
      "Epoch 50/100\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 660us/step - loss: 0.3697 - mae: 0.4746 - val_loss: 0.5322 - val_mae: 0.5601\n",
      "Epoch 51/100\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 663us/step - loss: 0.3470 - mae: 0.4617 - val_loss: 0.5285 - val_mae: 0.5563\n",
      "Epoch 52/100\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 663us/step - loss: 0.3656 - mae: 0.4665 - val_loss: 0.5564 - val_mae: 0.5703\n",
      "Epoch 53/100\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 647us/step - loss: 0.3484 - mae: 0.4640 - val_loss: 0.5329 - val_mae: 0.5603\n",
      "Epoch 54/100\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 640us/step - loss: 0.3756 - mae: 0.4845 - val_loss: 0.5344 - val_mae: 0.5579\n",
      "Epoch 55/100\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 648us/step - loss: 0.3490 - mae: 0.4643 - val_loss: 0.5347 - val_mae: 0.5614\n",
      "Epoch 56/100\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 636us/step - loss: 0.3350 - mae: 0.4558 - val_loss: 0.5474 - val_mae: 0.5687\n",
      "Epoch 57/100\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 632us/step - loss: 0.3555 - mae: 0.4668 - val_loss: 0.5274 - val_mae: 0.5547\n",
      "Epoch 58/100\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 651us/step - loss: 0.3423 - mae: 0.4521 - val_loss: 0.5476 - val_mae: 0.5691\n",
      "Epoch 59/100\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 635us/step - loss: 0.3529 - mae: 0.4602 - val_loss: 0.5289 - val_mae: 0.5573\n",
      "Epoch 60/100\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 640us/step - loss: 0.3440 - mae: 0.4554 - val_loss: 0.5252 - val_mae: 0.5546\n",
      "Epoch 61/100\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 632us/step - loss: 0.3234 - mae: 0.4496 - val_loss: 0.5553 - val_mae: 0.5679\n",
      "Epoch 62/100\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 629us/step - loss: 0.3522 - mae: 0.4654 - val_loss: 0.5509 - val_mae: 0.5771\n",
      "Epoch 63/100\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 667us/step - loss: 0.3489 - mae: 0.4581 - val_loss: 0.5363 - val_mae: 0.5637\n",
      "Epoch 64/100\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 699us/step - loss: 0.3324 - mae: 0.4485 - val_loss: 0.5455 - val_mae: 0.5669\n",
      "Epoch 65/100\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 751us/step - loss: 0.3466 - mae: 0.4638 - val_loss: 0.5372 - val_mae: 0.5615\n",
      "Epoch 66/100\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 792us/step - loss: 0.3566 - mae: 0.4652 - val_loss: 0.5352 - val_mae: 0.5619\n",
      "Epoch 67/100\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 650us/step - loss: 0.3255 - mae: 0.4472 - val_loss: 0.5551 - val_mae: 0.5632\n",
      "Epoch 68/100\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 696us/step - loss: 0.3512 - mae: 0.4611 - val_loss: 0.5603 - val_mae: 0.5728\n",
      "Epoch 69/100\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 718us/step - loss: 0.3486 - mae: 0.4641 - val_loss: 0.5215 - val_mae: 0.5501\n",
      "Epoch 70/100\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 709us/step - loss: 0.3180 - mae: 0.4429 - val_loss: 0.5297 - val_mae: 0.5580\n",
      "Epoch 71/100\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 934us/step - loss: 0.3393 - mae: 0.4561 - val_loss: 0.5421 - val_mae: 0.5633\n",
      "Epoch 72/100\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 687us/step - loss: 0.3255 - mae: 0.4457 - val_loss: 0.5216 - val_mae: 0.5494\n",
      "Epoch 73/100\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 682us/step - loss: 0.3216 - mae: 0.4438 - val_loss: 0.5433 - val_mae: 0.5655\n",
      "Epoch 74/100\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 686us/step - loss: 0.3295 - mae: 0.4476 - val_loss: 0.5428 - val_mae: 0.5609\n",
      "Epoch 75/100\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 864us/step - loss: 0.3455 - mae: 0.4585 - val_loss: 0.5801 - val_mae: 0.5847\n",
      "Epoch 76/100\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 721us/step - loss: 0.3342 - mae: 0.4492 - val_loss: 0.5294 - val_mae: 0.5550\n",
      "Epoch 77/100\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 666us/step - loss: 0.3209 - mae: 0.4452 - val_loss: 0.5825 - val_mae: 0.5854\n",
      "Epoch 78/100\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 671us/step - loss: 0.3223 - mae: 0.4445 - val_loss: 0.5524 - val_mae: 0.5686\n",
      "Epoch 79/100\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 665us/step - loss: 0.3112 - mae: 0.4350 - val_loss: 0.5344 - val_mae: 0.5578\n",
      "Epoch 80/100\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 688us/step - loss: 0.3159 - mae: 0.4373 - val_loss: 0.5326 - val_mae: 0.5560\n",
      "Epoch 81/100\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 716us/step - loss: 0.3185 - mae: 0.4432 - val_loss: 0.5481 - val_mae: 0.5672\n",
      "Epoch 82/100\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 694us/step - loss: 0.3251 - mae: 0.4458 - val_loss: 0.5719 - val_mae: 0.5815\n",
      "Epoch 83/100\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 692us/step - loss: 0.3169 - mae: 0.4395 - val_loss: 0.5431 - val_mae: 0.5644\n",
      "Epoch 84/100\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 669us/step - loss: 0.3116 - mae: 0.4345 - val_loss: 0.5460 - val_mae: 0.5674\n",
      "Epoch 85/100\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 686us/step - loss: 0.3031 - mae: 0.4335 - val_loss: 0.5384 - val_mae: 0.5609\n",
      "Epoch 86/100\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 698us/step - loss: 0.2955 - mae: 0.4227 - val_loss: 0.5375 - val_mae: 0.5588\n",
      "Epoch 87/100\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 956us/step - loss: 0.2949 - mae: 0.4237 - val_loss: 0.6170 - val_mae: 0.5972\n",
      "Epoch 88/100\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 673us/step - loss: 0.3008 - mae: 0.4332 - val_loss: 0.5399 - val_mae: 0.5617\n",
      "Epoch 89/100\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 669us/step - loss: 0.2937 - mae: 0.4270 - val_loss: 0.5457 - val_mae: 0.5653\n",
      "Epoch 90/100\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 668us/step - loss: 0.2929 - mae: 0.4208 - val_loss: 0.5360 - val_mae: 0.5580\n",
      "Epoch 91/100\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 664us/step - loss: 0.2792 - mae: 0.4151 - val_loss: 0.5745 - val_mae: 0.5739\n",
      "Epoch 92/100\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 731us/step - loss: 0.3052 - mae: 0.4322 - val_loss: 0.5424 - val_mae: 0.5597\n",
      "Epoch 93/100\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 858us/step - loss: 0.2905 - mae: 0.4177 - val_loss: 0.5322 - val_mae: 0.5577\n",
      "Epoch 94/100\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 759us/step - loss: 0.2951 - mae: 0.4256 - val_loss: 0.5569 - val_mae: 0.5736\n",
      "Epoch 95/100\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 664us/step - loss: 0.2826 - mae: 0.4111 - val_loss: 0.5893 - val_mae: 0.5937\n",
      "Epoch 96/100\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 762us/step - loss: 0.3061 - mae: 0.4330 - val_loss: 0.5457 - val_mae: 0.5606\n",
      "Epoch 97/100\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 869us/step - loss: 0.2853 - mae: 0.4173 - val_loss: 0.5523 - val_mae: 0.5683\n",
      "Epoch 98/100\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 690us/step - loss: 0.2772 - mae: 0.4109 - val_loss: 0.5519 - val_mae: 0.5701\n",
      "Epoch 99/100\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 694us/step - loss: 0.2976 - mae: 0.4267 - val_loss: 0.5499 - val_mae: 0.5616\n",
      "Epoch 100/100\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 644us/step - loss: 0.2686 - mae: 0.4084 - val_loss: 0.5489 - val_mae: 0.5643\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x178de6410>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=100, batch_size=16, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 722us/step - loss: 0.5539 - mae: 0.5711\n",
      "Model MAE: 0.5584\n"
     ]
    }
   ],
   "source": [
    "loss, mae = model.evaluate(X_test, y_test)\n",
    "print(f\"Model MAE: {mae:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I also ended up trying a few different epoch amounts and I seemed to have the best results whilst using 100 epochs.\n",
    "\n",
    "As can be seen here this dropped the MAE score from .57 to .55 which is encouraging to see as it was still fitting well beforehand.\n",
    "\n",
    "I am now going to try this on the red wine dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X = red_df.drop('quality', axis=1)\n",
    "y = red_df['quality']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential([\n",
    "    keras.layers.Input(shape=(X_train.shape[1],)),\n",
    "    keras.layers.Dense(64, activation='relu'),\n",
    "    keras.layers.Dense(32, activation='relu'),\n",
    "    keras.layers.Dense(1, activation='linear')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='mse', metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 27.6665 - mae: 5.0815 - val_loss: 4.7705 - val_mae: 1.9128\n",
      "Epoch 2/100\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.0794 - mae: 1.4374 - val_loss: 2.0831 - val_mae: 1.1222\n",
      "Epoch 3/100\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.1349 - mae: 1.1196 - val_loss: 1.7931 - val_mae: 1.0498\n",
      "Epoch 4/100\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 969us/step - loss: 1.7248 - mae: 1.0241 - val_loss: 1.6002 - val_mae: 0.9924\n",
      "Epoch 5/100\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.4155 - mae: 0.9429 - val_loss: 1.4399 - val_mae: 0.9326\n",
      "Epoch 6/100\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.2647 - mae: 0.8827 - val_loss: 1.2695 - val_mae: 0.8813\n",
      "Epoch 7/100\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 984us/step - loss: 1.0398 - mae: 0.8043 - val_loss: 1.1428 - val_mae: 0.8269\n",
      "Epoch 8/100\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 994us/step - loss: 1.0090 - mae: 0.7943 - val_loss: 1.0298 - val_mae: 0.7878\n",
      "Epoch 9/100\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.8240 - mae: 0.7210 - val_loss: 0.9358 - val_mae: 0.7460\n",
      "Epoch 10/100\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 993us/step - loss: 0.8480 - mae: 0.7299 - val_loss: 0.8707 - val_mae: 0.7347\n",
      "Epoch 11/100\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 972us/step - loss: 0.7314 - mae: 0.6666 - val_loss: 0.7584 - val_mae: 0.6693\n",
      "Epoch 12/100\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 991us/step - loss: 0.6894 - mae: 0.6501 - val_loss: 0.7365 - val_mae: 0.6685\n",
      "Epoch 13/100\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 979us/step - loss: 0.6197 - mae: 0.6132 - val_loss: 0.6522 - val_mae: 0.6162\n",
      "Epoch 14/100\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 987us/step - loss: 0.5580 - mae: 0.5820 - val_loss: 0.6195 - val_mae: 0.6041\n",
      "Epoch 15/100\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 997us/step - loss: 0.5284 - mae: 0.5706 - val_loss: 0.5843 - val_mae: 0.5929\n",
      "Epoch 16/100\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 988us/step - loss: 0.4944 - mae: 0.5510 - val_loss: 0.5541 - val_mae: 0.5831\n",
      "Epoch 17/100\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 989us/step - loss: 0.5229 - mae: 0.5744 - val_loss: 0.5146 - val_mae: 0.5398\n",
      "Epoch 18/100\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 991us/step - loss: 0.4684 - mae: 0.5335 - val_loss: 0.4888 - val_mae: 0.5288\n",
      "Epoch 19/100\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.4780 - mae: 0.5398 - val_loss: 0.4682 - val_mae: 0.5326\n",
      "Epoch 20/100\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 934us/step - loss: 0.4329 - mae: 0.5156 - val_loss: 0.4541 - val_mae: 0.5171\n",
      "Epoch 21/100\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 996us/step - loss: 0.4067 - mae: 0.4997 - val_loss: 0.4530 - val_mae: 0.5150\n",
      "Epoch 22/100\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 931us/step - loss: 0.3962 - mae: 0.4862 - val_loss: 0.4519 - val_mae: 0.5211\n",
      "Epoch 23/100\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 922us/step - loss: 0.4010 - mae: 0.5022 - val_loss: 0.4436 - val_mae: 0.5155\n",
      "Epoch 24/100\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.4566 - mae: 0.5309 - val_loss: 0.4224 - val_mae: 0.5024\n",
      "Epoch 25/100\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 987us/step - loss: 0.3867 - mae: 0.4874 - val_loss: 0.4424 - val_mae: 0.5237\n",
      "Epoch 26/100\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.3965 - mae: 0.5013 - val_loss: 0.4291 - val_mae: 0.5069\n",
      "Epoch 27/100\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 996us/step - loss: 0.3855 - mae: 0.4858 - val_loss: 0.4081 - val_mae: 0.5011\n",
      "Epoch 28/100\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 991us/step - loss: 0.3651 - mae: 0.4771 - val_loss: 0.4062 - val_mae: 0.4908\n",
      "Epoch 29/100\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 983us/step - loss: 0.3844 - mae: 0.4867 - val_loss: 0.3971 - val_mae: 0.4940\n",
      "Epoch 30/100\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 935us/step - loss: 0.3473 - mae: 0.4588 - val_loss: 0.4176 - val_mae: 0.4975\n",
      "Epoch 31/100\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.3816 - mae: 0.4913 - val_loss: 0.3924 - val_mae: 0.4851\n",
      "Epoch 32/100\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 986us/step - loss: 0.3716 - mae: 0.4751 - val_loss: 0.3816 - val_mae: 0.4755\n",
      "Epoch 33/100\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 914us/step - loss: 0.3529 - mae: 0.4530 - val_loss: 0.4030 - val_mae: 0.4845\n",
      "Epoch 34/100\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 932us/step - loss: 0.3979 - mae: 0.4901 - val_loss: 0.3849 - val_mae: 0.4746\n",
      "Epoch 35/100\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 919us/step - loss: 0.3412 - mae: 0.4570 - val_loss: 0.3791 - val_mae: 0.4746\n",
      "Epoch 36/100\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 984us/step - loss: 0.3489 - mae: 0.4552 - val_loss: 0.3650 - val_mae: 0.4674\n",
      "Epoch 37/100\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 980us/step - loss: 0.3221 - mae: 0.4388 - val_loss: 0.3874 - val_mae: 0.4806\n",
      "Epoch 38/100\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 927us/step - loss: 0.3445 - mae: 0.4512 - val_loss: 0.3631 - val_mae: 0.4606\n",
      "Epoch 39/100\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 989us/step - loss: 0.3705 - mae: 0.4682 - val_loss: 0.3924 - val_mae: 0.4771\n",
      "Epoch 40/100\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 979us/step - loss: 0.3148 - mae: 0.4354 - val_loss: 0.3791 - val_mae: 0.4752\n",
      "Epoch 41/100\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.3369 - mae: 0.4465 - val_loss: 0.3909 - val_mae: 0.4853\n",
      "Epoch 42/100\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.3248 - mae: 0.4501 - val_loss: 0.3972 - val_mae: 0.4908\n",
      "Epoch 43/100\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 992us/step - loss: 0.3379 - mae: 0.4516 - val_loss: 0.3739 - val_mae: 0.4636\n",
      "Epoch 44/100\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 983us/step - loss: 0.3036 - mae: 0.4233 - val_loss: 0.3734 - val_mae: 0.4687\n",
      "Epoch 45/100\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 995us/step - loss: 0.3117 - mae: 0.4375 - val_loss: 0.3928 - val_mae: 0.4876\n",
      "Epoch 46/100\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 918us/step - loss: 0.3277 - mae: 0.4468 - val_loss: 0.3882 - val_mae: 0.4758\n",
      "Epoch 47/100\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3147 - mae: 0.4366 - val_loss: 0.3898 - val_mae: 0.4759\n",
      "Epoch 48/100\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 984us/step - loss: 0.2997 - mae: 0.4225 - val_loss: 0.4157 - val_mae: 0.4970\n",
      "Epoch 49/100\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 925us/step - loss: 0.3050 - mae: 0.4350 - val_loss: 0.3627 - val_mae: 0.4622\n",
      "Epoch 50/100\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 939us/step - loss: 0.3240 - mae: 0.4380 - val_loss: 0.4057 - val_mae: 0.4843\n",
      "Epoch 51/100\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 989us/step - loss: 0.3008 - mae: 0.4193 - val_loss: 0.3632 - val_mae: 0.4712\n",
      "Epoch 52/100\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 995us/step - loss: 0.2943 - mae: 0.4220 - val_loss: 0.4489 - val_mae: 0.5154\n",
      "Epoch 53/100\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 985us/step - loss: 0.3250 - mae: 0.4395 - val_loss: 0.3870 - val_mae: 0.4801\n",
      "Epoch 54/100\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 988us/step - loss: 0.2833 - mae: 0.4170 - val_loss: 0.3836 - val_mae: 0.4768\n",
      "Epoch 55/100\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 960us/step - loss: 0.3187 - mae: 0.4368 - val_loss: 0.3902 - val_mae: 0.4883\n",
      "Epoch 56/100\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 992us/step - loss: 0.3238 - mae: 0.4399 - val_loss: 0.3731 - val_mae: 0.4641\n",
      "Epoch 57/100\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.2694 - mae: 0.4043 - val_loss: 0.3605 - val_mae: 0.4500\n",
      "Epoch 58/100\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.2582 - mae: 0.3923 - val_loss: 0.3610 - val_mae: 0.4571\n",
      "Epoch 59/100\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.2672 - mae: 0.4035 - val_loss: 0.3858 - val_mae: 0.4755\n",
      "Epoch 60/100\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.2943 - mae: 0.4196 - val_loss: 0.3663 - val_mae: 0.4546\n",
      "Epoch 61/100\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.3024 - mae: 0.4222 - val_loss: 0.3650 - val_mae: 0.4599\n",
      "Epoch 62/100\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.2474 - mae: 0.3899 - val_loss: 0.3659 - val_mae: 0.4544\n",
      "Epoch 63/100\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 997us/step - loss: 0.2745 - mae: 0.4076 - val_loss: 0.3543 - val_mae: 0.4515\n",
      "Epoch 64/100\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.2952 - mae: 0.4198 - val_loss: 0.3775 - val_mae: 0.4714\n",
      "Epoch 65/100\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.2786 - mae: 0.4060 - val_loss: 0.3914 - val_mae: 0.4862\n",
      "Epoch 66/100\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.2663 - mae: 0.3966 - val_loss: 0.3668 - val_mae: 0.4578\n",
      "Epoch 67/100\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.2717 - mae: 0.3911 - val_loss: 0.3771 - val_mae: 0.4643\n",
      "Epoch 68/100\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.2548 - mae: 0.3878 - val_loss: 0.3711 - val_mae: 0.4611\n",
      "Epoch 69/100\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.2663 - mae: 0.3947 - val_loss: 0.3612 - val_mae: 0.4578\n",
      "Epoch 70/100\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.2810 - mae: 0.4078 - val_loss: 0.4048 - val_mae: 0.4826\n",
      "Epoch 71/100\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 950us/step - loss: 0.2899 - mae: 0.4181 - val_loss: 0.4525 - val_mae: 0.5155\n",
      "Epoch 72/100\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.2780 - mae: 0.3990 - val_loss: 0.4233 - val_mae: 0.4987\n",
      "Epoch 73/100\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 948us/step - loss: 0.2621 - mae: 0.3937 - val_loss: 0.3689 - val_mae: 0.4687\n",
      "Epoch 74/100\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 932us/step - loss: 0.2464 - mae: 0.3856 - val_loss: 0.4029 - val_mae: 0.4841\n",
      "Epoch 75/100\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 990us/step - loss: 0.2763 - mae: 0.4112 - val_loss: 0.3995 - val_mae: 0.4738\n",
      "Epoch 76/100\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 931us/step - loss: 0.2574 - mae: 0.3902 - val_loss: 0.3838 - val_mae: 0.4719\n",
      "Epoch 77/100\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 979us/step - loss: 0.2652 - mae: 0.3917 - val_loss: 0.4049 - val_mae: 0.4869\n",
      "Epoch 78/100\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 982us/step - loss: 0.2340 - mae: 0.3729 - val_loss: 0.3734 - val_mae: 0.4606\n",
      "Epoch 79/100\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 919us/step - loss: 0.2408 - mae: 0.3796 - val_loss: 0.3895 - val_mae: 0.4698\n",
      "Epoch 80/100\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 991us/step - loss: 0.2319 - mae: 0.3609 - val_loss: 0.3825 - val_mae: 0.4672\n",
      "Epoch 81/100\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 990us/step - loss: 0.2441 - mae: 0.3786 - val_loss: 0.3945 - val_mae: 0.4764\n",
      "Epoch 82/100\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 994us/step - loss: 0.2503 - mae: 0.3794 - val_loss: 0.4267 - val_mae: 0.5026\n",
      "Epoch 83/100\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 988us/step - loss: 0.2414 - mae: 0.3818 - val_loss: 0.4494 - val_mae: 0.5066\n",
      "Epoch 84/100\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 963us/step - loss: 0.2613 - mae: 0.3986 - val_loss: 0.3870 - val_mae: 0.4607\n",
      "Epoch 85/100\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 977us/step - loss: 0.2229 - mae: 0.3582 - val_loss: 0.4057 - val_mae: 0.4864\n",
      "Epoch 86/100\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.2322 - mae: 0.3709 - val_loss: 0.3940 - val_mae: 0.4708\n",
      "Epoch 87/100\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.2221 - mae: 0.3614 - val_loss: 0.3933 - val_mae: 0.4657\n",
      "Epoch 88/100\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.2296 - mae: 0.3586 - val_loss: 0.3871 - val_mae: 0.4696\n",
      "Epoch 89/100\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 996us/step - loss: 0.2172 - mae: 0.3607 - val_loss: 0.4095 - val_mae: 0.4847\n",
      "Epoch 90/100\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 935us/step - loss: 0.2189 - mae: 0.3585 - val_loss: 0.4341 - val_mae: 0.4872\n",
      "Epoch 91/100\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 988us/step - loss: 0.2091 - mae: 0.3510 - val_loss: 0.3861 - val_mae: 0.4631\n",
      "Epoch 92/100\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 995us/step - loss: 0.2273 - mae: 0.3653 - val_loss: 0.4007 - val_mae: 0.4823\n",
      "Epoch 93/100\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 963us/step - loss: 0.2156 - mae: 0.3646 - val_loss: 0.4272 - val_mae: 0.4979\n",
      "Epoch 94/100\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 980us/step - loss: 0.2044 - mae: 0.3464 - val_loss: 0.4197 - val_mae: 0.4910\n",
      "Epoch 95/100\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 998us/step - loss: 0.2180 - mae: 0.3585 - val_loss: 0.4065 - val_mae: 0.4744\n",
      "Epoch 96/100\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1000us/step - loss: 0.2059 - mae: 0.3484 - val_loss: 0.4263 - val_mae: 0.4861\n",
      "Epoch 97/100\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 977us/step - loss: 0.2012 - mae: 0.3508 - val_loss: 0.4398 - val_mae: 0.5188\n",
      "Epoch 98/100\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 932us/step - loss: 0.2423 - mae: 0.3793 - val_loss: 0.4421 - val_mae: 0.4939\n",
      "Epoch 99/100\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 978us/step - loss: 0.2166 - mae: 0.3596 - val_loss: 0.4107 - val_mae: 0.4681\n",
      "Epoch 100/100\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 929us/step - loss: 0.2160 - mae: 0.3512 - val_loss: 0.4443 - val_mae: 0.5055\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x17b166210>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=100, batch_size=16, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.4428 - mae: 0.5176 \n",
      "Model MAE: 0.5184\n"
     ]
    }
   ],
   "source": [
    "loss, mae = model.evaluate(X_test, y_test)\n",
    "print(f\"Model MAE: {mae:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen by the code block above the red wine is predicting better as it has a MAE rating of 0.51 which is better than the white wine rating, I now want to combine the 2 datasets and see the results of this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = pd.concat([white_df, red_df])\n",
    "\n",
    "combined_df.to_csv('winequality-combined.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I'm just combining both of the datasets and then saving them to a new csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X = combined_df.drop('quality', axis=1)\n",
    "y = combined_df['quality']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential([\n",
    "    keras.layers.Input(shape=(X_train.shape[1],)),\n",
    "    keras.layers.Dense(64, activation='relu'),\n",
    "    keras.layers.Dense(32, activation='relu'),\n",
    "    keras.layers.Dense(1, activation='linear')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='mse', metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 864us/step - loss: 13.7221 - mae: 3.0679 - val_loss: 1.5738 - val_mae: 0.9810\n",
      "Epoch 2/100\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 661us/step - loss: 1.5737 - mae: 0.9689 - val_loss: 1.1133 - val_mae: 0.8191\n",
      "Epoch 3/100\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 653us/step - loss: 1.0925 - mae: 0.7970 - val_loss: 0.8281 - val_mae: 0.7094\n",
      "Epoch 4/100\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 634us/step - loss: 0.7683 - mae: 0.6830 - val_loss: 0.6570 - val_mae: 0.6211\n",
      "Epoch 5/100\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 631us/step - loss: 0.6175 - mae: 0.6167 - val_loss: 0.5960 - val_mae: 0.5962\n",
      "Epoch 6/100\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 630us/step - loss: 0.5330 - mae: 0.5702 - val_loss: 0.5485 - val_mae: 0.5697\n",
      "Epoch 7/100\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 626us/step - loss: 0.5323 - mae: 0.5689 - val_loss: 0.5450 - val_mae: 0.5669\n",
      "Epoch 8/100\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 630us/step - loss: 0.5275 - mae: 0.5646 - val_loss: 0.5397 - val_mae: 0.5626\n",
      "Epoch 9/100\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 634us/step - loss: 0.4803 - mae: 0.5443 - val_loss: 0.7002 - val_mae: 0.6488\n",
      "Epoch 10/100\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 637us/step - loss: 0.4754 - mae: 0.5400 - val_loss: 0.5377 - val_mae: 0.5632\n",
      "Epoch 11/100\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 627us/step - loss: 0.4853 - mae: 0.5422 - val_loss: 0.5041 - val_mae: 0.5416\n",
      "Epoch 12/100\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 651us/step - loss: 0.4745 - mae: 0.5337 - val_loss: 0.5102 - val_mae: 0.5460\n",
      "Epoch 13/100\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 622us/step - loss: 0.4549 - mae: 0.5251 - val_loss: 0.5061 - val_mae: 0.5408\n",
      "Epoch 14/100\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 625us/step - loss: 0.4793 - mae: 0.5405 - val_loss: 0.5054 - val_mae: 0.5442\n",
      "Epoch 15/100\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 624us/step - loss: 0.4649 - mae: 0.5368 - val_loss: 0.5266 - val_mae: 0.5587\n",
      "Epoch 16/100\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 630us/step - loss: 0.4617 - mae: 0.5280 - val_loss: 0.5204 - val_mae: 0.5510\n",
      "Epoch 17/100\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 622us/step - loss: 0.4609 - mae: 0.5339 - val_loss: 0.5055 - val_mae: 0.5430\n",
      "Epoch 18/100\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 634us/step - loss: 0.4392 - mae: 0.5189 - val_loss: 0.5387 - val_mae: 0.5642\n",
      "Epoch 19/100\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 629us/step - loss: 0.4514 - mae: 0.5217 - val_loss: 0.5108 - val_mae: 0.5396\n",
      "Epoch 20/100\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 650us/step - loss: 0.4299 - mae: 0.5152 - val_loss: 0.5423 - val_mae: 0.5680\n",
      "Epoch 21/100\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 627us/step - loss: 0.4300 - mae: 0.5160 - val_loss: 0.5388 - val_mae: 0.5643\n",
      "Epoch 22/100\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 648us/step - loss: 0.4287 - mae: 0.5193 - val_loss: 0.5101 - val_mae: 0.5461\n",
      "Epoch 23/100\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 637us/step - loss: 0.4297 - mae: 0.5169 - val_loss: 0.5177 - val_mae: 0.5539\n",
      "Epoch 24/100\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 736us/step - loss: 0.4248 - mae: 0.5073 - val_loss: 0.5252 - val_mae: 0.5584\n",
      "Epoch 25/100\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 986us/step - loss: 0.4176 - mae: 0.5069 - val_loss: 0.5048 - val_mae: 0.5448\n",
      "Epoch 26/100\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 619us/step - loss: 0.4184 - mae: 0.5051 - val_loss: 0.5340 - val_mae: 0.5609\n",
      "Epoch 27/100\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 644us/step - loss: 0.4288 - mae: 0.5138 - val_loss: 0.5207 - val_mae: 0.5517\n",
      "Epoch 28/100\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 608us/step - loss: 0.4246 - mae: 0.5097 - val_loss: 0.5009 - val_mae: 0.5396\n",
      "Epoch 29/100\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 602us/step - loss: 0.4110 - mae: 0.5008 - val_loss: 0.5754 - val_mae: 0.5841\n",
      "Epoch 30/100\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 605us/step - loss: 0.4298 - mae: 0.5133 - val_loss: 0.5297 - val_mae: 0.5543\n",
      "Epoch 31/100\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 605us/step - loss: 0.4309 - mae: 0.5107 - val_loss: 0.5119 - val_mae: 0.5486\n",
      "Epoch 32/100\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 614us/step - loss: 0.4171 - mae: 0.5055 - val_loss: 0.4930 - val_mae: 0.5359\n",
      "Epoch 33/100\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 630us/step - loss: 0.4186 - mae: 0.5088 - val_loss: 0.5006 - val_mae: 0.5421\n",
      "Epoch 34/100\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 624us/step - loss: 0.4021 - mae: 0.4970 - val_loss: 0.5262 - val_mae: 0.5606\n",
      "Epoch 35/100\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 619us/step - loss: 0.4068 - mae: 0.4983 - val_loss: 0.4976 - val_mae: 0.5362\n",
      "Epoch 36/100\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 606us/step - loss: 0.4018 - mae: 0.4966 - val_loss: 0.5246 - val_mae: 0.5578\n",
      "Epoch 37/100\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 610us/step - loss: 0.4117 - mae: 0.5009 - val_loss: 0.4906 - val_mae: 0.5286\n",
      "Epoch 38/100\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 599us/step - loss: 0.3941 - mae: 0.4914 - val_loss: 0.5229 - val_mae: 0.5497\n",
      "Epoch 39/100\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 598us/step - loss: 0.3905 - mae: 0.4899 - val_loss: 0.5045 - val_mae: 0.5463\n",
      "Epoch 40/100\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 592us/step - loss: 0.3937 - mae: 0.4965 - val_loss: 0.5133 - val_mae: 0.5468\n",
      "Epoch 41/100\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 600us/step - loss: 0.4088 - mae: 0.5011 - val_loss: 0.5242 - val_mae: 0.5585\n",
      "Epoch 42/100\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 614us/step - loss: 0.3815 - mae: 0.4804 - val_loss: 0.5354 - val_mae: 0.5613\n",
      "Epoch 43/100\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 597us/step - loss: 0.3991 - mae: 0.4947 - val_loss: 0.5380 - val_mae: 0.5678\n",
      "Epoch 44/100\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 621us/step - loss: 0.3905 - mae: 0.4908 - val_loss: 0.5334 - val_mae: 0.5599\n",
      "Epoch 45/100\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 630us/step - loss: 0.3969 - mae: 0.4878 - val_loss: 0.5068 - val_mae: 0.5404\n",
      "Epoch 46/100\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 610us/step - loss: 0.3908 - mae: 0.4899 - val_loss: 0.5315 - val_mae: 0.5604\n",
      "Epoch 47/100\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 813us/step - loss: 0.3969 - mae: 0.4929 - val_loss: 0.5003 - val_mae: 0.5399\n",
      "Epoch 48/100\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 603us/step - loss: 0.3825 - mae: 0.4859 - val_loss: 0.5013 - val_mae: 0.5404\n",
      "Epoch 49/100\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 600us/step - loss: 0.3820 - mae: 0.4825 - val_loss: 0.5157 - val_mae: 0.5493\n",
      "Epoch 50/100\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 593us/step - loss: 0.3805 - mae: 0.4821 - val_loss: 0.4987 - val_mae: 0.5438\n",
      "Epoch 51/100\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 600us/step - loss: 0.3843 - mae: 0.4800 - val_loss: 0.4859 - val_mae: 0.5335\n",
      "Epoch 52/100\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 595us/step - loss: 0.3773 - mae: 0.4754 - val_loss: 0.5411 - val_mae: 0.5600\n",
      "Epoch 53/100\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 642us/step - loss: 0.3710 - mae: 0.4725 - val_loss: 0.5063 - val_mae: 0.5420\n",
      "Epoch 54/100\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 607us/step - loss: 0.3743 - mae: 0.4801 - val_loss: 0.5546 - val_mae: 0.5650\n",
      "Epoch 55/100\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 612us/step - loss: 0.3757 - mae: 0.4834 - val_loss: 0.4967 - val_mae: 0.5369\n",
      "Epoch 56/100\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 622us/step - loss: 0.3763 - mae: 0.4836 - val_loss: 0.4981 - val_mae: 0.5403\n",
      "Epoch 57/100\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 626us/step - loss: 0.3575 - mae: 0.4670 - val_loss: 0.5047 - val_mae: 0.5394\n",
      "Epoch 58/100\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 681us/step - loss: 0.3569 - mae: 0.4689 - val_loss: 0.5015 - val_mae: 0.5445\n",
      "Epoch 59/100\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 594us/step - loss: 0.3661 - mae: 0.4770 - val_loss: 0.5591 - val_mae: 0.5761\n",
      "Epoch 60/100\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 597us/step - loss: 0.3778 - mae: 0.4823 - val_loss: 0.5104 - val_mae: 0.5495\n",
      "Epoch 61/100\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 599us/step - loss: 0.3644 - mae: 0.4675 - val_loss: 0.5217 - val_mae: 0.5444\n",
      "Epoch 62/100\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 593us/step - loss: 0.3778 - mae: 0.4814 - val_loss: 0.5155 - val_mae: 0.5554\n",
      "Epoch 63/100\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 631us/step - loss: 0.3671 - mae: 0.4772 - val_loss: 0.5037 - val_mae: 0.5425\n",
      "Epoch 64/100\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 777us/step - loss: 0.3606 - mae: 0.4647 - val_loss: 0.5074 - val_mae: 0.5442\n",
      "Epoch 65/100\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 620us/step - loss: 0.3625 - mae: 0.4713 - val_loss: 0.5002 - val_mae: 0.5409\n",
      "Epoch 66/100\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 625us/step - loss: 0.3554 - mae: 0.4658 - val_loss: 0.5255 - val_mae: 0.5476\n",
      "Epoch 67/100\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 595us/step - loss: 0.3628 - mae: 0.4741 - val_loss: 0.5034 - val_mae: 0.5456\n",
      "Epoch 68/100\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 602us/step - loss: 0.3612 - mae: 0.4715 - val_loss: 0.5156 - val_mae: 0.5452\n",
      "Epoch 69/100\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 594us/step - loss: 0.3597 - mae: 0.4652 - val_loss: 0.5291 - val_mae: 0.5615\n",
      "Epoch 70/100\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 590us/step - loss: 0.3557 - mae: 0.4679 - val_loss: 0.5478 - val_mae: 0.5622\n",
      "Epoch 71/100\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 593us/step - loss: 0.3383 - mae: 0.4550 - val_loss: 0.5194 - val_mae: 0.5577\n",
      "Epoch 72/100\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 636us/step - loss: 0.3545 - mae: 0.4681 - val_loss: 0.5354 - val_mae: 0.5644\n",
      "Epoch 73/100\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 623us/step - loss: 0.3648 - mae: 0.4687 - val_loss: 0.5034 - val_mae: 0.5394\n",
      "Epoch 74/100\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 600us/step - loss: 0.3357 - mae: 0.4538 - val_loss: 0.5178 - val_mae: 0.5507\n",
      "Epoch 75/100\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 832us/step - loss: 0.3535 - mae: 0.4593 - val_loss: 0.5240 - val_mae: 0.5591\n",
      "Epoch 76/100\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 608us/step - loss: 0.3488 - mae: 0.4567 - val_loss: 0.5172 - val_mae: 0.5535\n",
      "Epoch 77/100\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 601us/step - loss: 0.3399 - mae: 0.4538 - val_loss: 0.5095 - val_mae: 0.5494\n",
      "Epoch 78/100\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 605us/step - loss: 0.3389 - mae: 0.4548 - val_loss: 0.4944 - val_mae: 0.5302\n",
      "Epoch 79/100\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 668us/step - loss: 0.3497 - mae: 0.4614 - val_loss: 0.5429 - val_mae: 0.5617\n",
      "Epoch 80/100\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 689us/step - loss: 0.3632 - mae: 0.4733 - val_loss: 0.5116 - val_mae: 0.5439\n",
      "Epoch 81/100\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 747us/step - loss: 0.3463 - mae: 0.4527 - val_loss: 0.5056 - val_mae: 0.5429\n",
      "Epoch 82/100\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 655us/step - loss: 0.3698 - mae: 0.4660 - val_loss: 0.5280 - val_mae: 0.5564\n",
      "Epoch 83/100\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 691us/step - loss: 0.3431 - mae: 0.4557 - val_loss: 0.5490 - val_mae: 0.5741\n",
      "Epoch 84/100\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 610us/step - loss: 0.3544 - mae: 0.4658 - val_loss: 0.5376 - val_mae: 0.5618\n",
      "Epoch 85/100\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 606us/step - loss: 0.3508 - mae: 0.4614 - val_loss: 0.5127 - val_mae: 0.5430\n",
      "Epoch 86/100\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 890us/step - loss: 0.3335 - mae: 0.4477 - val_loss: 0.5315 - val_mae: 0.5642\n",
      "Epoch 87/100\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 629us/step - loss: 0.3301 - mae: 0.4475 - val_loss: 0.5080 - val_mae: 0.5460\n",
      "Epoch 88/100\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 610us/step - loss: 0.3166 - mae: 0.4425 - val_loss: 0.5078 - val_mae: 0.5460\n",
      "Epoch 89/100\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 672us/step - loss: 0.3383 - mae: 0.4533 - val_loss: 0.5136 - val_mae: 0.5488\n",
      "Epoch 90/100\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 670us/step - loss: 0.3456 - mae: 0.4606 - val_loss: 0.5277 - val_mae: 0.5511\n",
      "Epoch 91/100\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 611us/step - loss: 0.3388 - mae: 0.4516 - val_loss: 0.5104 - val_mae: 0.5459\n",
      "Epoch 92/100\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 595us/step - loss: 0.3272 - mae: 0.4508 - val_loss: 0.5050 - val_mae: 0.5447\n",
      "Epoch 93/100\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 603us/step - loss: 0.3357 - mae: 0.4495 - val_loss: 0.5482 - val_mae: 0.5715\n",
      "Epoch 94/100\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 659us/step - loss: 0.3320 - mae: 0.4474 - val_loss: 0.5197 - val_mae: 0.5484\n",
      "Epoch 95/100\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 618us/step - loss: 0.3286 - mae: 0.4469 - val_loss: 0.5118 - val_mae: 0.5449\n",
      "Epoch 96/100\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 641us/step - loss: 0.3313 - mae: 0.4506 - val_loss: 0.5401 - val_mae: 0.5663\n",
      "Epoch 97/100\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 679us/step - loss: 0.3378 - mae: 0.4542 - val_loss: 0.5109 - val_mae: 0.5477\n",
      "Epoch 98/100\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 855us/step - loss: 0.3320 - mae: 0.4455 - val_loss: 0.5107 - val_mae: 0.5420\n",
      "Epoch 99/100\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 654us/step - loss: 0.3457 - mae: 0.4564 - val_loss: 0.5051 - val_mae: 0.5413\n",
      "Epoch 100/100\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 652us/step - loss: 0.3355 - mae: 0.4512 - val_loss: 0.5129 - val_mae: 0.5445\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x178dd63d0>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=100, batch_size=16, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 731us/step - loss: 0.4374 - mae: 0.5128\n",
      "Model MAE: 0.5122\n"
     ]
    }
   ],
   "source": [
    "loss, mae = model.evaluate(X_test, y_test)\n",
    "print(f\"Model MAE: {mae:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The combined dataset is also performing well similarly to the red wine as it is sitting at 0.51 on the MAE score."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
